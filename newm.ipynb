{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Samples Directory: True\n",
      "Dirty Samples Directory: True\n",
      "Clean Samples: ['29.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '36.jpg', '37.jpg', '38.jpg', '39.jpg', 'istockphoto-1164176852-612x612.jpg', 'istockphoto-1323511566-612x612.jpg', 'istockphoto-92657257-612x612.jpg']\n",
      "Dirty Samples: ['15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to datasets\n",
    "test_data_dir = r\"C:\\Users\\purvp\\Desktop\\Dataset water\\test\"\n",
    "clean_samples_dir = os.path.join(test_data_dir, \"Clean-samples\")\n",
    "dirty_samples_dir = os.path.join(test_data_dir, \"Dirty-samples\")\n",
    "\n",
    "# Check if directories exist\n",
    "print(\"Clean Samples Directory:\", os.path.exists(clean_samples_dir))\n",
    "print(\"Dirty Samples Directory:\", os.path.exists(dirty_samples_dir))\n",
    "\n",
    "# List files in directories\n",
    "print(\"Clean Samples:\", os.listdir(clean_samples_dir))\n",
    "print(\"Dirty Samples:\", os.listdir(dirty_samples_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 409 images belonging to 2 classes.\n",
      "Found 17 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purvp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purvp\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 369ms/step - accuracy: 0.5396 - loss: 0.9888 - val_accuracy: 0.7059 - val_loss: 0.6501\n",
      "Epoch 2/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6875 - loss: 0.6417 - val_accuracy: 0.8235 - val_loss: 0.5615\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 334ms/step - accuracy: 0.7048 - loss: 0.5822 - val_accuracy: 0.6471 - val_loss: 0.5349\n",
      "Epoch 4/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6875 - loss: 0.5753 - val_accuracy: 0.6471 - val_loss: 0.5189\n",
      "Epoch 5/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 325ms/step - accuracy: 0.6720 - loss: 0.5883 - val_accuracy: 0.8235 - val_loss: 0.5852\n",
      "Epoch 6/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7188 - loss: 0.5730 - val_accuracy: 0.7647 - val_loss: 0.6123\n",
      "Epoch 7/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 321ms/step - accuracy: 0.7382 - loss: 0.5338 - val_accuracy: 0.7059 - val_loss: 0.5455\n",
      "Epoch 8/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6562 - loss: 0.6917 - val_accuracy: 0.6471 - val_loss: 0.4879\n",
      "Epoch 9/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 310ms/step - accuracy: 0.7285 - loss: 0.5291 - val_accuracy: 0.8235 - val_loss: 0.3360\n",
      "Epoch 10/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7500 - loss: 0.5986 - val_accuracy: 0.7647 - val_loss: 0.3503\n",
      "Epoch 11/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step - accuracy: 0.7767 - loss: 0.4895 - val_accuracy: 0.7059 - val_loss: 0.4514\n",
      "Epoch 12/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8125 - loss: 0.4235 - val_accuracy: 0.7059 - val_loss: 0.4451\n",
      "Epoch 13/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 314ms/step - accuracy: 0.8003 - loss: 0.4067 - val_accuracy: 0.8824 - val_loss: 0.2894\n",
      "Epoch 14/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8125 - loss: 0.4403 - val_accuracy: 0.9412 - val_loss: 0.3081\n",
      "Epoch 15/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317ms/step - accuracy: 0.8109 - loss: 0.4561 - val_accuracy: 0.8235 - val_loss: 0.4291\n",
      "Epoch 16/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8125 - loss: 0.3977 - val_accuracy: 0.8235 - val_loss: 0.3336\n",
      "Epoch 17/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step - accuracy: 0.8127 - loss: 0.4294 - val_accuracy: 0.8824 - val_loss: 0.2595\n",
      "Epoch 18/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8400 - loss: 0.3880 - val_accuracy: 0.8824 - val_loss: 0.2533\n",
      "Epoch 19/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 314ms/step - accuracy: 0.8326 - loss: 0.3759 - val_accuracy: 0.9412 - val_loss: 0.1988\n",
      "Epoch 20/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7812 - loss: 0.3812 - val_accuracy: 0.8824 - val_loss: 0.2349\n",
      "1/1 - 0s - 86ms/step - accuracy: 0.8824 - loss: 0.2349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define paths to the dataset\n",
    "train_dir = r\"C:\\Users\\purvp\\Desktop\\Dataset water\\train\"\n",
    "test_dir = r\"C:\\Users\\purvp\\Desktop\\Dataset water\\test\"\n",
    "\n",
    "# Image parameters\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Only rescaling for validation/testing\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(validation_generator, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"water_purity_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 822ms/step - accuracy: 0.5859 - loss: 1.4449 - val_accuracy: 0.7059 - val_loss: 0.8843\n",
      "Epoch 2/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.7500 - loss: 0.8940 - val_accuracy: 0.5882 - val_loss: 0.7175\n",
      "Epoch 3/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 776ms/step - accuracy: 0.7517 - loss: 0.5919 - val_accuracy: 0.7647 - val_loss: 0.5167\n",
      "Epoch 4/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.8125 - loss: 0.4709 - val_accuracy: 0.7647 - val_loss: 0.4319\n",
      "Epoch 5/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 766ms/step - accuracy: 0.8051 - loss: 0.4380 - val_accuracy: 0.8824 - val_loss: 0.2957\n",
      "Epoch 6/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.8125 - loss: 0.3769 - val_accuracy: 0.8824 - val_loss: 0.2789\n",
      "Epoch 7/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 757ms/step - accuracy: 0.8669 - loss: 0.3089 - val_accuracy: 0.9412 - val_loss: 0.2100\n",
      "Epoch 8/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.9688 - loss: 0.2698 - val_accuracy: 1.0000 - val_loss: 0.2026\n",
      "Epoch 9/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 771ms/step - accuracy: 0.9024 - loss: 0.2968 - val_accuracy: 0.9412 - val_loss: 0.1810\n",
      "Epoch 10/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9062 - loss: 0.2479 - val_accuracy: 0.9412 - val_loss: 0.1818\n",
      "Epoch 11/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 784ms/step - accuracy: 0.8825 - loss: 0.2545 - val_accuracy: 1.0000 - val_loss: 0.1701\n",
      "Epoch 12/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9062 - loss: 0.2778 - val_accuracy: 1.0000 - val_loss: 0.1628\n",
      "Epoch 13/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 789ms/step - accuracy: 0.9212 - loss: 0.2254 - val_accuracy: 1.0000 - val_loss: 0.1416\n",
      "Epoch 14/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9375 - loss: 0.1880 - val_accuracy: 1.0000 - val_loss: 0.1378\n",
      "Epoch 15/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 810ms/step - accuracy: 0.9183 - loss: 0.2116 - val_accuracy: 1.0000 - val_loss: 0.1227\n",
      "Epoch 16/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.8438 - loss: 0.2975 - val_accuracy: 1.0000 - val_loss: 0.1136\n",
      "Epoch 17/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 843ms/step - accuracy: 0.9434 - loss: 0.1720 - val_accuracy: 1.0000 - val_loss: 0.1153\n",
      "Epoch 18/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.8000 - loss: 0.4179 - val_accuracy: 1.0000 - val_loss: 0.1218\n",
      "Epoch 19/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 820ms/step - accuracy: 0.9395 - loss: 0.1631 - val_accuracy: 1.0000 - val_loss: 0.1567\n",
      "Epoch 20/20\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9062 - loss: 0.2381 - val_accuracy: 1.0000 - val_loss: 0.1689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Load the VGG16 model with pre-trained weights, excluding the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"water_purity_model_vgg16.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 images belonging to 2 classes.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Original Model Evaluation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Clean-samples       1.00      0.83      0.91        12\n",
      "Dirty-samples       0.71      1.00      0.83         5\n",
      "\n",
      "     accuracy                           0.88        17\n",
      "    macro avg       0.86      0.92      0.87        17\n",
      " weighted avg       0.92      0.88      0.89        17\n",
      "\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "VGG16 Model Evaluation:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Clean-samples       1.00      1.00      1.00        12\n",
      "Dirty-samples       1.00      1.00      1.00         5\n",
      "\n",
      "     accuracy                           1.00        17\n",
      "    macro avg       1.00      1.00      1.00        17\n",
      " weighted avg       1.00      1.00      1.00        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Paths to datasets\n",
    "test_data_dir = r\"C:\\Users\\purvp\\Desktop\\Dataset water\\test\"\n",
    "\n",
    "# Load the models\n",
    "model_original = load_model(\"water_purity_model.h5\")\n",
    "model_vgg16 = load_model(\"water_purity_model_vgg16.h5\")\n",
    "\n",
    "# Image data generator for test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Check the steps per epoch calculation\n",
    "steps_per_epoch = max(1, test_generator.samples // test_generator.batch_size)\n",
    "\n",
    "# Evaluate the original model\n",
    "test_generator.reset()\n",
    "preds_original = model_original.predict(test_generator, steps=steps_per_epoch, verbose=1)\n",
    "y_true = test_generator.classes\n",
    "y_pred_original = np.where(preds_original > 0.5, 1, 0)\n",
    "\n",
    "print(\"Original Model Evaluation:\")\n",
    "print(classification_report(y_true, y_pred_original, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Evaluate the VGG16 model\n",
    "test_generator.reset()\n",
    "preds_vgg16 = model_vgg16.predict(test_generator, steps=steps_per_epoch, verbose=1)\n",
    "y_pred_vgg16 = np.where(preds_vgg16 > 0.5, 1, 0)\n",
    "\n",
    "print(\"VGG16 Model Evaluation:\")\n",
    "print(classification_report(y_true, y_pred_vgg16, target_names=test_generator.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the models\n",
    "model_original = load_model(\"water_purity_model.h5\")\n",
    "model_vgg16 = load_model(\"water_purity_model_vgg16.h5\")\n",
    "\n",
    "def predict_image(image_path, model):\n",
    "    img = load_img(image_path, target_size=(150, 150))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    prediction = model.predict(img_array)\n",
    "    return 'Dirty' if prediction > 0.5 else 'Clean'\n",
    "\n",
    "def open_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((250, 250))\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        panel.config(image=img)\n",
    "        panel.image = img\n",
    "\n",
    "        # Reset result labels\n",
    "        result_label_original.config(text=\"\")\n",
    "        result_label_vgg16.config(text=\"\")\n",
    "        result_label_comparison.config(text=\"\")\n",
    "\n",
    "        selected_model = model_var.get()\n",
    "        \n",
    "        if selected_model == \"Original Model\":\n",
    "            result_original = predict_image(file_path, model_original)\n",
    "            result_label_original.config(text=f\"Original Model: {result_original}\")\n",
    "        elif selected_model == \"VGG16 Model\":\n",
    "            result_vgg16 = predict_image(file_path, model_vgg16)\n",
    "            result_label_vgg16.config(text=f\"VGG16 Model: {result_vgg16}\")\n",
    "        elif selected_model == \"Compare Both\":\n",
    "            result_original = predict_image(file_path, model_original)\n",
    "            result_vgg16 = predict_image(file_path, model_vgg16)\n",
    "            result_label_original.config(text=f\"Original Model: {result_original}\")\n",
    "            result_label_vgg16.config(text=f\"VGG16 Model: {result_vgg16}\")\n",
    "            comparison = \"Match\" if result_original == result_vgg16 else \"Mismatch\"\n",
    "            result_label_comparison.config(text=f\"Comparison: {comparison}\")\n",
    "\n",
    "# Main application\n",
    "root = tk.Tk()\n",
    "root.title(\"Water Purity Prediction\")\n",
    "root.geometry(\"600x600\")\n",
    "root.resizable(False, False)\n",
    "\n",
    "# Title Label\n",
    "title_label = ttk.Label(root, text=\"Water Purity Prediction\", font=(\"Helvetica\", 18))\n",
    "title_label.pack(pady=10)\n",
    "\n",
    "# Image Display Panel\n",
    "panel = ttk.Label(root)\n",
    "panel.pack(pady=10)\n",
    "\n",
    "# Model Selection\n",
    "model_var = tk.StringVar(value=\"Original Model\")\n",
    "model_label = ttk.Label(root, text=\"Select Model:\")\n",
    "model_label.pack(pady=5)\n",
    "model_frame = ttk.Frame(root)\n",
    "model_frame.pack(pady=5)\n",
    "\n",
    "original_model_rb = ttk.Radiobutton(model_frame, text=\"Original Model\", variable=model_var, value=\"Original Model\")\n",
    "vgg16_model_rb = ttk.Radiobutton(model_frame, text=\"VGG16 Model\", variable=model_var, value=\"VGG16 Model\")\n",
    "compare_rb = ttk.Radiobutton(model_frame, text=\"Compare Both\", variable=model_var, value=\"Compare Both\")\n",
    "\n",
    "original_model_rb.grid(row=0, column=0, padx=5, pady=5)\n",
    "vgg16_model_rb.grid(row=0, column=1, padx=5, pady=5)\n",
    "compare_rb.grid(row=0, column=2, padx=5, pady=5)\n",
    "\n",
    "# Upload Button\n",
    "upload_button = ttk.Button(root, text=\"Upload Image\", command=open_file)\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Result Labels\n",
    "result_label_original = ttk.Label(root, text=\"\", font=(\"Helvetica\", 12))\n",
    "result_label_original.pack(pady=5)\n",
    "\n",
    "result_label_vgg16 = ttk.Label(root, text=\"\", font=(\"Helvetica\", 12))\n",
    "result_label_vgg16.pack(pady=5)\n",
    "\n",
    "result_label_comparison = ttk.Label(root, text=\"\", font=(\"Helvetica\", 12))\n",
    "result_label_comparison.pack(pady=5)\n",
    "\n",
    "# Start the GUI event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
